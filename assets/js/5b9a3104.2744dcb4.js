"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[208],{4785:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"Module-4-VLA/Week-12/intro","title":"1. Vision-Language-Action","description":"VLA Architecture","source":"@site/docs/04-Module-4-VLA/01-Week-12/01-intro.mdx","sourceDirName":"04-Module-4-VLA/01-Week-12","slug":"/Module-4-VLA/Week-12/intro","permalink":"/Hackathon/docs/Module-4-VLA/Week-12/intro","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"1. Vision-Language-Action","description":"VLA Architecture"},"sidebar":"tutorialSidebar","previous":{"title":"2. Lab: Security Patrol","permalink":"/Hackathon/docs/Module-3-AI-Robot-Brain/Week-11/lab-patrol"},"next":{"title":"2. Lab: Voice Control","permalink":"/Hackathon/docs/Module-4-VLA/Week-12/lab-voice"}}');var i=t(4848),r=t(8453);const s={sidebar_position:1,title:"1. Vision-Language-Action",description:"VLA Architecture"},c="Vision-Language-Action (VLA)",l={},a=[{value:"The Concept",id:"the-concept",level:2},{value:"Models",id:"models",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"})}),"\n",(0,i.jsx)(n.h2,{id:"the-concept",children:"The Concept"}),"\n",(0,i.jsxs)(n.p,{children:["Traditional robotics pipeline:\n",(0,i.jsx)(n.code,{children:"Perception -> State Estimation -> Planning -> Control"})]}),"\n",(0,i.jsxs)(n.p,{children:["VLA pipeline:\n",(0,i.jsx)(n.code,{children:"Image + Text Prompt -> VLA Model -> Robot Action"})]}),"\n",(0,i.jsx)(n.h2,{id:"models",children:"Models"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RT-2 (Google)"}),": Robotic Transformer."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenVLA"}),": Open source alternative."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'These models understand semantic commands like "Pick up the plush toy" or "Move the spoon near the bowl".'})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>c});var o=t(6540);const i={},r=o.createContext(i);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);